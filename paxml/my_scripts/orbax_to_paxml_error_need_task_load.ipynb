{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83cf9c9d-2566-4975-8815-41e1000a1700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: distributed.initialize should only be called once.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import orbax\n",
    "from optax import MaskedNode\n",
    "from etils import epath\n",
    "\n",
    "from praxis import base_hyperparams\n",
    "from praxis import pax_fiddle\n",
    "from praxis import py_utils\n",
    "from paxml import checkpoints  # mapped to internal\n",
    "from paxml import checkpoint_managers\n",
    "from paxml import train_states\n",
    "from paxml import trainer_lib\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "\n",
    "sys.path.append('/home/lishengping/projects/paxml/paxml')\n",
    "\n",
    "from paxml.main import get_experiment\n",
    "\n",
    "\n",
    "try:\n",
    "    jax.distributed.initialize()\n",
    "except Exception as error:\n",
    "    print(f'Error: {error}')\n",
    "    assert jax.local_device_count() == 8\n",
    "    \n",
    "\n",
    "TrainState = train_states.TrainState\n",
    "instantiate = base_hyperparams.instantiate\n",
    "CheckpointType = checkpoints.CheckpointType\n",
    "Checkpointer = checkpoints.Checkpointer\n",
    "PaxCheckpointHandler = checkpoints.PaxCheckpointHandler\n",
    "NestedMap = py_utils.NestedMap\n",
    "\n",
    "\n",
    "experiment_config = get_experiment('tasks.lm.params.c4.C4SpmdGpt37BRoPE')()\n",
    "task_p = experiment_config.task()\n",
    "jax_task = instantiate(task_p)\n",
    "\n",
    "SAVE_INTERVAL_STEPS = 1\n",
    "options = checkpoint_managers.CheckpointManagerOptions(\n",
    "      max_to_keep=10,\n",
    "      save_interval_steps=SAVE_INTERVAL_STEPS,\n",
    "      cleanup_tmp_directories=True,\n",
    "  )\n",
    "\n",
    "checkpointer = Checkpointer(\n",
    "          PaxCheckpointHandler(\n",
    "              enforce_restore_shape_check=False,\n",
    "              use_ocdbt=False,\n",
    "          )\n",
    "      )\n",
    "\n",
    "job_log_dir = epath.Path('gs://llm_projects/log/lspdebug0804/checkpoints')\n",
    "# job_log_dir = epath.Path('gs://llm_base_models/baichuan-7B-easylm')\n",
    "\n",
    "checkpoint_type = CheckpointType.GDA\n",
    "\n",
    "checkpoint_manager = checkpoint_managers.OrbaxCheckpointManager(\n",
    "      job_log_dir,\n",
    "      checkpointer,\n",
    "      train_input_checkpointer=False,\n",
    "      options=options,\n",
    "      checkpoint_type=checkpoint_type,\n",
    "      tensorstore_use_ocdbt=False,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3db3db49-d2af-45b7-b4cd-f0a0c9c58e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start load pretrained model params....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1691307112.818550   65699 gcs_resource.cc:97] Using default AdmissionQueue with limit 32\n",
      "I0000 00:00:1691307112.822667   75620 google_auth_provider.cc:179] Running on GCE, using service account 97048824446-compute@developer.gserviceaccount.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model params finished, take time: 6.7192864418029785s.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(f'Start load pretrained model params....')\n",
    "gold_mngr_dir = epath.Path('gs://llm_base_models/baichuan-7B-easylm')\n",
    "gold_mngr_dir = epath.Path('gs://llm_base_models/orbax_async_test')\n",
    "gold_item = {\n",
    "            # 'opt_state': orbax.checkpoint.AsyncCheckpointer(orbax.checkpoint.PyTreeCheckpointHandler()),\n",
    "            'params': orbax.checkpoint.AsyncCheckpointer(orbax.checkpoint.PyTreeCheckpointHandler()),\n",
    "            # 'step': orbax.checkpoint.AsyncCheckpointer(orbax.checkpoint.ArrayCheckpointHandler()),\n",
    "                }\n",
    "gold_mngr = orbax.checkpoint.CheckpointManager(gold_mngr_dir, gold_item)\n",
    "\n",
    "with jax.default_device(jax.devices(\"cpu\")[0]):\n",
    "    gold_w = gold_mngr.restore(gold_mngr.latest_step())\n",
    "    \n",
    "print(f'Load pretrained model params finished, take time: {time.time() - start}s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "304e72be-5ce5-44d5-8319-bdd9e7032035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please simple check model shape and dtype...\n",
      "('params', 'lm', 'embedding_lookup', 'emb_var') (32000, 4096) float32\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'ffn_layer1', 'linear', 'w') (2, 4096, 11008) float32\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'ffn_layer1_gate', 'linear', 'w') (2, 4096, 11008) float32\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'ffn_layer2', 'linear', 'w') (2, 11008, 4096) float32\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'query', 'w') (2, 4096, 32, 128) float32\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'key', 'w') (2, 4096, 32, 128) float32\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'value', 'w') (2, 4096, 32, 128) float32\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'post', 'w') (2, 4096, 32, 128) float32\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'layer_norm', 'scale') (2, 4096) float32\n",
      "('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'layer_norm', 'scale') (2, 4096) float32\n",
      "('params', 'lm', 'final_ln', 'scale') (4096,) float32\n",
      "('params', 'lm', 'softmax', 'logits_ffn', 'linear', 'w') (4096, 32000) float32\n"
     ]
    }
   ],
   "source": [
    "paxml_to_mesh_format = {\n",
    "        ('params', 'lm', 'embedding_lookup', 'emb_var'): 'wte',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'ffn_layer1', 'linear', 'w'): 'w3',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'ffn_layer1_gate', 'linear', 'w'): 'w1',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'ffn_layer2', 'linear', 'w'): 'w2',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'query', 'w'): 'wq',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'key', 'w'): 'wk',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'value', 'w'): 'wv',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'self_attention', 'post', 'w'): 'wo',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'layer_norm', 'scale'): 'attention_norm',\n",
    "        ('params', 'lm', 'transformer', 'repeat', 'sub', 'x_layers_0', 'ff_layer', 'layer_norm', 'scale'): 'ffn_norm',\n",
    "        ('params', 'lm', 'final_ln', 'scale'): 'ln_f',\n",
    "        ('params', 'lm', 'softmax', 'logits_ffn', 'linear', 'w'): 'lm_head',\n",
    "    }\n",
    "\n",
    "num_heads = experiment_config.NUM_HEADS\n",
    "model_dims = experiment_config.MODEL_DIMS \n",
    "head_dim = model_dims // num_heads\n",
    "\n",
    "trans_result = {}\n",
    "with jax.default_device(jax.devices(\"cpu\")[0]):\n",
    "    for k, v in paxml_to_mesh_format.items():\n",
    "        values = []\n",
    "        for gold_key, glod_values in flatten_dict(gold_w['params']).items():\n",
    "            if v in gold_key:\n",
    "                if v in 'wqwkwvwo':\n",
    "                    glod_values = glod_values.reshape(model_dims, num_heads, head_dim)\n",
    "                values.append([gold_key, glod_values])\n",
    "        values = sorted(values, key=lambda x: x[0])\n",
    "        if len(values) > 1:\n",
    "            stack_values = np.stack(list(zip(*values))[1])\n",
    "        else:\n",
    "            stack_values = values[0][1]\n",
    "        trans_result[k] = stack_values\n",
    "    opt_state_mv = jax.tree_map(lambda x: jnp.zeros_like(x), trans_result)\n",
    "\n",
    "print(f'Please simple check model shape and dtype...')\n",
    "for k, v in trans_result.items():\n",
    "    print(k, v.shape, v.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1376514-b218-4eef-ac16-88df0654880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model save step is 1320\n",
      "Saved model finished. take time: 26.856893062591553s !!!\n",
      "Args check_saved_model_fail_or_success is True, start to check model whether saved successful...\n",
      "Start load model to check whether saved model is True or False\n",
      "Check model finished. model is  saved successfully. take time: 9.500797033309937\n"
     ]
    }
   ],
   "source": [
    "latest_step =  checkpoint_manager.latest_step()\n",
    "step = latest_step + SAVE_INTERVAL_STEPS if latest_step is not None else SAVE_INTERVAL_STEPS\n",
    "print(f'Model save step is {step}')\n",
    "n_layers = experiment_config.NUM_LAYERS # 模型的层数\n",
    "# n_layers = 32 # 模型的层数\n",
    "check_saved_model_fail_or_success = True\n",
    "start = time.time()\n",
    "temp_no_prefix, temp_other = {}, {}\n",
    "for key_tuple, param in opt_state_mv.items():\n",
    "    if 'repeat' in key_tuple:\n",
    "        temp_no_prefix[key_tuple] = MaskedNode()\n",
    "        temp_other[key_tuple] = param\n",
    "    else:\n",
    "        temp_no_prefix[key_tuple] = param\n",
    "        temp_other[key_tuple] = MaskedNode()\n",
    "\n",
    "temp_no_prefix = unflatten_dict(temp_no_prefix)\n",
    "temp_other = unflatten_dict(temp_other)\n",
    "    \n",
    "no_prefix = {'count': jnp.array(step), 'm': temp_no_prefix, 'v': temp_no_prefix}\n",
    "other = {'count': jnp.array([step] * n_layers), 'm': temp_other, 'v': temp_other}\n",
    "trans_opt_states = {\n",
    "    'no_prefix': [{'count': jnp.array(step)}] * 2 + [no_prefix, {'count': jnp.array(step)}], \n",
    "    f'p#{n_layers}#i-1': [{'count': jnp.array([step] * n_layers)}] * 2 + [other, {'count': jnp.array([step] * n_layers)}], \n",
    "}\n",
    "trans_opt_states = [trans_opt_states]\n",
    "\n",
    "\n",
    "new_trainstate = TrainState(\n",
    "                            step=jnp.array(step), \n",
    "                            mdl_vars=unflatten_dict(trans_result),\n",
    "                            opt_states=trans_opt_states\n",
    ")\n",
    "padded_global_shapes = jax.tree_map(lambda x: jax.ShapeDtypeStruct(shape=x.shape, dtype=x.dtype) \n",
    "                                    if hasattr(x, 'shape') else x , new_trainstate)\n",
    "checkpoint_manager.save(step, new_trainstate, padded_global_shapes, train_input_pipeline=None, force=False)\n",
    "print(f'Saved model finished. take time: {time.time() - start}s !!!')\n",
    "\n",
    "if check_saved_model_fail_or_success:\n",
    "    start = time.time()\n",
    "    print(f'Args check_saved_model_fail_or_success is {check_saved_model_fail_or_success}, start to check model whether saved successful...')\n",
    "    # fake输入只是为了拿到dtype和shape\n",
    "    seed = 0\n",
    "    jax.random.PRNGKey(seed)\n",
    "    low, high = 0, experiment_config.VOCAB_SIZE\n",
    "    seq_length = 10\n",
    "    # batch_size = experiment_config.PERCORE_BATCH_SIZE * 8\n",
    "    batch_size = 1\n",
    "    my_sample_input = {}\n",
    "    my_sample_input['ids'] = np.random.randint(low, high, (batch_size, seq_length)).astype(np.int32)\n",
    "    my_sample_input['labels'] = my_sample_input['ids'].astype(np.int32)\n",
    "    my_sample_input['weights'] = np.ones((batch_size, seq_length)).astype(np.float32)\n",
    "    my_sample_input['paddings'] = my_sample_input['weights']\n",
    "    my_sample_input['segment_ids'] = my_sample_input['weights'].astype(np.int32)\n",
    "    my_sample_input['segment_pos'] = np.arange(seq_length).reshape(1, -1).repeat(batch_size, axis=0).astype(np.int32)\n",
    "    my_sample_input['_seqio_provenance/shard_index'] = np.array([-1]).repeat(batch_size).astype(np.int32)\n",
    "    my_sample_input['_seqio_provenance/num_shards'] = my_sample_input['_seqio_provenance/shard_index']\n",
    "    my_sample_input['_seqio_provenance/index_within_shard'] = my_sample_input['_seqio_provenance/shard_index'].astype(np.int64)\n",
    "    my_sample_input['eval_sample_weights'] = my_sample_input['_seqio_provenance/shard_index'].astype(np.float32)\n",
    "    my_sample_input = NestedMap(my_sample_input)\n",
    "    \n",
    "    inputs_shape_dtype = jax.tree_map(\n",
    "            lambda x: jax.ShapeDtypeStruct(shape=x.shape, dtype=x.dtype),\n",
    "            my_sample_input,\n",
    "        )\n",
    "    train_state_metadata = trainer_lib.create_train_state_metadata(\n",
    "        jax_task,\n",
    "        inputs_shape_dtype,\n",
    "        discard_opt_states=False,\n",
    "        do_eval=True,\n",
    "    )\n",
    "    print(f'Start load model to check whether saved model is True or False')\n",
    "    device_mesh = py_utils.create_device_mesh(\n",
    "          jax_task.model.ici_mesh_shape,\n",
    "          jax_task.model.dcn_mesh_shape,\n",
    "          contiguous_submeshes=jax_task.model.contiguous_submeshes,\n",
    "      )\n",
    "    global_mesh = jax.sharding.Mesh(device_mesh, jax_task.model.mesh_axis_names)\n",
    "    restore_kwargs = {\n",
    "              'version': 1.1,\n",
    "              'specs': train_state_metadata.partition_specs, # shard\n",
    "              'mesh': global_mesh, # mesh\n",
    "              'transforms': None, # None\n",
    "          }\n",
    "    restore_kwargs = {'state': restore_kwargs}\n",
    "    items = {'state': train_state_metadata.padded_global_shapes}\n",
    "    restored_model = checkpoint_manager._manager.restore(step, items=items, restore_kwargs=restore_kwargs)\n",
    "    print(f'Check model finished. model is  saved successfully. take time: {time.time() - start}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
